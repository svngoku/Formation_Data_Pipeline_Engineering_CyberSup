# ğŸš€ Formation Data Pipeline - Starter Kit

> Pipeline de donnÃ©es de bout en bout avec n8n, Airflow, dbt et Prefect

[![CI](https://github.com/your-org/data-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/your-org/data-pipeline/actions/workflows/ci.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![dbt](https://img.shields.io/badge/dbt-1.7+-orange.svg)](https://www.getdbt.com/)

## ğŸ“‹ PrÃ©requis

- **Docker** et **Docker Compose** (v2+)
- **Python 3.11+**
- **Git**
- **8 GB RAM** minimum (Airflow est gourmand)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Architecture Pipeline                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   API   â”‚â”€â”€â”€â”€â–ºâ”‚   n8n       â”‚â”€â”€â”€â”€â–ºâ”‚ Airflow â”‚â”€â”€â”€â”€â–ºâ”‚   dbt    â”‚  â”‚
â”‚  â”‚ Source  â”‚     â”‚ (triggers)  â”‚     â”‚ (orch)  â”‚     â”‚ (transf) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                           â”‚                â”‚        â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                         â–¼                                            â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚                  â”‚   DuckDB    â”‚                                     â”‚
â”‚                  â”‚  Warehouse  â”‚                                     â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                       â”‚
â”‚  Data Flow:                                                          â”‚
â”‚  [Bronze: raw Parquet] â†’ [Silver: cleaned] â†’ [Gold: aggregated]    â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Cloner et configurer

```bash
git clone https://github.com/your-org/data-pipeline.git
cd data-pipeline

# Copier la configuration
cp .env.example .env

# Installer les dÃ©pendances Python
pip install -e ".[dev]"
```

### 2. Lancer l'infrastructure

```bash
# DÃ©marrer tous les services
docker-compose -f infra/docker-compose.yml up -d

# VÃ©rifier les services
docker-compose -f infra/docker-compose.yml ps
```

### 3. AccÃ©der aux interfaces

| Service | URL | Credentials |
|---------|-----|-------------|
| **n8n** | http://localhost:5678 | (crÃ©er au premier accÃ¨s) |
| **Airflow** | http://localhost:8080 | admin / admin |
| **dbt Docs** | http://localhost:8001 | - |

### 4. Premier test

```bash
# ExÃ©cuter le script d'ingestion
python scripts/ingest_api.py --start-date 2024-01-01 --end-date 2024-01-01

# Valider les donnÃ©es
python scripts/validate_data.py --input-file data/bronze/partition_date=2024-01-01/*.parquet

# Lancer dbt
cd dbt && dbt run && dbt test
```

## ğŸ“ Structure du projet

```
data-pipeline/
â”œâ”€â”€ ğŸ“‚ airflow/
â”‚   â””â”€â”€ dags/                # DAGs Airflow
â”œâ”€â”€ ğŸ“‚ dbt/                  # Projet dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/         # Silver layer
â”‚   â”‚   â””â”€â”€ marts/           # Gold layer
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ ğŸ“‚ docs/                 # Documentation dÃ©taillÃ©e
â”‚   â”œâ”€â”€ JOUR-1_*.md
â”‚   â”œâ”€â”€ JOUR-2_*.md
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ runbook.md
â”œâ”€â”€ ğŸ“‚ infra/                # Docker & infra
â”œâ”€â”€ ğŸ“‚ n8n/                  # Workflows n8n
â”œâ”€â”€ ğŸ“‚ prefect/              # Flows Prefect
â”œâ”€â”€ ğŸ“‚ scripts/              # Scripts Python
â”œâ”€â”€ ğŸ“‚ tests/                # Tests unitaires/intÃ©gration
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md                # Ce fichier
```

## ğŸ› ï¸ Commandes utiles

```bash
# --- Infrastructure ---
make run-airflow          # DÃ©marrer Airflow
make stop-airflow         # ArrÃªter Airflow

# --- DÃ©veloppement ---
make lint                 # VÃ©rifier le code
make format               # Formater le code
make test                 # Tests unitaires
make test-integration     # Tests d'intÃ©gration

# --- dbt ---
make run-dbt              # ExÃ©cuter les modÃ¨les
make test-dbt             # Lancer les tests dbt
make docs-dbt             # GÃ©nÃ©rer et servir la doc
```

## ğŸ“… Programme de la formation

| Jour | ThÃ¨me | Contenu |
|------|-------|---------|
| **J1** | n8n & Fondations | Automatisation low-code, triggers, workflows |
| **J2** | Ingestion | Patterns incrÃ©mentaux, scripts Python, Parquet |
| **J3** | Airflow | DAGs, operators, scheduling, backfill |
| **J4** | dbt & Prefect | Transformations SQL, ELT, orchestration Python |
| **J5** | Production | CI/CD, tests, documentation, projet final |

## ğŸ”§ Configuration

### Variables d'environnement (.env)

```bash
# API
API_BASE_URL=https://jsonplaceholder.typicode.com
API_TIMEOUT=30

# Paths
DATA_DIR=./data
DBT_DATABASE_PATH=./data/warehouse.duckdb

# Alerting (optionnel)
SLACK_WEBHOOK=
ALERT_EMAIL=
```

### Profil dbt (~/.dbt/profiles.yml)

```yaml
dbt_training:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: "data/warehouse.duckdb"
      threads: 4
```

## ğŸ“Š Livrables attendus

Ã€ la fin de la formation, votre repo doit contenir :

- [ ] Scripts d'ingestion et validation fonctionnels
- [ ] Workflow n8n exportÃ©
- [ ] DAG Airflow opÃ©rationnel
- [ ] Projet dbt avec tests
- [ ] Documentation complÃ¨te
- [ ] CI/CD configurÃ© (bonus)

## ğŸ†˜ Troubleshooting

### Airflow ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose -f infra/docker-compose.yml logs airflow-scheduler

# RÃ©initialiser la base
docker-compose -f infra/docker-compose.yml down -v
docker-compose -f infra/docker-compose.yml up -d
```

### dbt ne trouve pas les sources
```bash
# VÃ©rifier le chemin dans sources.yml
# S'assurer que les fichiers Parquet existent
ls -la data/bronze/
```

### n8n "Execute Command" Ã©choue
```bash
# VÃ©rifier que le script est exÃ©cutable
docker exec n8n chmod +x /home/node/scripts/*.py

# Tester manuellement
docker exec n8n python3 /home/node/scripts/ingest_api.py --help
```

## ğŸ“š Ressources

- [Documentation n8n](https://docs.n8n.io/)
- [Apache Airflow](https://airflow.apache.org/docs/)
- [dbt Documentation](https://docs.getdbt.com/)
- [Prefect Documentation](https://docs.prefect.io/)
- [DuckDB](https://duckdb.org/docs/)

## ğŸ‘¥ Contributeurs

Formation crÃ©Ã©e par [Votre Nom] - [votre-email@example.com]

## ğŸ“„ License

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.
