# Jour 2 ‚Äî n8n + Scripting pour l'ingestion data

> **Dur√©e totale** : 7h  
> **Objectif** : Ma√Ætriser les patterns d'ingestion incr√©mentale et structurer la logique m√©tier dans des scripts versionn√©s

---

## üåÖ MATIN (3h30)

### 08:30 - 08:45 | R√©activation Jour 1 (15 min)
- Questions sur les TPs de la veille
- Retour sur les exercices optionnels
- D√©blocage si n√©cessaire

### 08:45 - 10:15 | Bloc Th√©orique : "Automatiser l'ingestion incr√©mentale" (1h30)

#### Partie 1 : Patterns d'ingestion (40 min)

**Full Load vs Incremental**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FULL LOAD                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Source ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ñ∫ Target   ‚îÇ
‚îÇ         (tout, √† chaque run)                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚úÖ Simple                    ‚ùå Co√ªteux en ressources      ‚îÇ
‚îÇ  ‚úÖ Donn√©es toujours fra√Æches ‚ùå Lent sur gros volumes      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INCREMENTAL LOAD                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Source ‚îÄ‚îÄ[Œî depuis last_run]‚îÄ‚îÄ‚ñ∫ Target                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Strat√©gies de watermark:                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ last_updated_at  ‚îÇ Colonne timestamp modifi√©e     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ last_id          ‚îÇ ID auto-incr√©ment√©             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ cursor/token     ‚îÇ Token de pagination API        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ hash/checksum    ‚îÇ D√©tection de changement        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pagination API**
```python
# Offset/Limit (simple mais probl√©matique si donn√©es changent)
GET /api/events?offset=0&limit=100
GET /api/events?offset=100&limit=100

# Cursor-based (recommand√©)
GET /api/events?cursor=abc123&limit=100
# Response: { data: [...], next_cursor: "def456" }

# Keyset pagination (performant)
GET /api/events?after_id=1000&limit=100
```

**Gestion des erreurs : Pattern DLQ**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Source  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Ingestion  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    Target    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ √©chec
                       ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Dead Letter    ‚îÇ  ‚Üê Quarantaine
              ‚îÇ     Queue       ‚îÇ  ‚Üê Retry manuel
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Partie 2 : Architecture "n8n + Scripts" (30 min)

**S√©paration des responsabilit√©s**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    n8n (Orchestrateur)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Scheduling (cron, webhooks)                             ‚îÇ
‚îÇ  ‚Ä¢ Gestion du state (last_run, watermarks)                 ‚îÇ
‚îÇ  ‚Ä¢ Routage et branchements                                 ‚îÇ
‚îÇ  ‚Ä¢ Notifications (succ√®s/√©chec)                            ‚îÇ
‚îÇ  ‚Ä¢ Interface visuelle pour ops                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Scripts Python/Bash (Logique)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Appels API avec retry/backoff                           ‚îÇ
‚îÇ  ‚Ä¢ Transformation des donn√©es                              ‚îÇ
‚îÇ  ‚Ä¢ Validation et qualit√©                                   ‚îÇ
‚îÇ  ‚Ä¢ √âcriture fichiers (Parquet, CSV)                        ‚îÇ
‚îÇ  ‚Ä¢ Logs structur√©s                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avantages scripts versionn√©s**
| Crit√®re | Script dans n8n | Script Git |
|---------|-----------------|------------|
| Versioning | ‚ùå | ‚úÖ Git history |
| Tests | ‚ùå | ‚úÖ pytest/unittest |
| Code review | ‚ùå | ‚úÖ PR/MR |
| R√©utilisation | ‚ùå | ‚úÖ Import modules |
| IDE support | ‚ùå | ‚úÖ Autocomplete, lint |

#### Partie 3 : Formats de donn√©es (20 min)

**Comparaison formats**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Format  ‚îÇ Lisibilit√© ‚îÇ Compression‚îÇ Schema        ‚îÇ Use case     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ JSON    ‚îÇ ‚úÖ Haute   ‚îÇ ‚ùå Aucune  ‚îÇ ‚ùå Flexible   ‚îÇ APIs, debug  ‚îÇ
‚îÇ CSV     ‚îÇ ‚úÖ Haute   ‚îÇ ‚ùå Faible  ‚îÇ ‚ùå Implicite  ‚îÇ Export, Excel‚îÇ
‚îÇ Parquet ‚îÇ ‚ùå Binaire ‚îÇ ‚úÖ Forte   ‚îÇ ‚úÖ Embarqu√©   ‚îÇ Analytics    ‚îÇ
‚îÇ Avro    ‚îÇ ‚ùå Binaire ‚îÇ ‚úÖ Moyenne ‚îÇ ‚úÖ √âvolutif   ‚îÇ Streaming    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pourquoi Parquet pour Bronze/Silver ?**
```python
# Exemple : m√™me dataset
import pandas as pd

df = pd.read_json("events.json")  # 100 MB
df.to_csv("events.csv")            # 120 MB
df.to_parquet("events.parquet")    # 15 MB  ‚Üê 8x plus petit!

# Lecture s√©lective (column pruning)
pd.read_parquet("events.parquet", columns=["id", "timestamp"])
```

### 10:15 - 10:30 | ‚òï Pause (15 min)

### 10:30 - 11:30 | Option : Pr√©sentation Node-RED (1h)

> **Note** : Section optionnelle, peut √™tre remplac√©e par plus de pratique n8n

#### Comparaison rapide n8n vs Node-RED

| Aspect | n8n | Node-RED |
|--------|-----|----------|
| Focus | Automation, integrations | IoT, message flows |
| Interface | Modern, UX soign√©e | Fonctionnelle |
| Nodes natifs | ~300 (SaaS focus) | ~5000 (hardware focus) |
| D√©ploiement | Docker, Cloud | Edge, Raspberry Pi |
| Communaut√© | Startup | IBM, industrie |
| Licence | Fair-code | Apache 2.0 |

#### D√©mo rapide (30 min)
```bash
docker run -it -p 1880:1880 nodered/node-red
```

**Workflow exemple** :
```
[inject] ‚Üí [http request] ‚Üí [json] ‚Üí [function] ‚Üí [debug]
```

#### Discussion : Quand choisir Node-RED ? (30 min)
- IoT et edge computing
- Protocoles industriels (MQTT, Modbus)
- Raspberry Pi / embarqu√©
- Flows temps r√©el

### 11:30 - 12:00 | Checkpoint matin (30 min)

**Exercice pratique** : Concevoir sur papier
> "Vous devez ing√©rer des transactions bancaires depuis une API pagin√©e. 
> Dessinez le flow et identifiez : watermark, gestion d'erreur, format de sortie."

**Discussion collective** des solutions propos√©es.

---

## üçΩÔ∏è PAUSE D√âJEUNER (12:00 - 13:30)

---

## üåÜ APR√àS-MIDI (3h30)

### 13:30 - 15:30 | TP2.1 : Ingestion API ‚Üí Parquet (2h)

#### Contexte
> Ing√©rer des √©v√©nements depuis une API pagin√©e (simulation avec JSONPlaceholder ou API publique)

#### Script `ingest_api.py` - Construction guid√©e

**√âtape 1 : Structure de base (20 min)**
```python
#!/usr/bin/env python3
"""
Ingestion incr√©mentale depuis une API pagin√©e.
Usage: python ingest_api.py --start-date 2024-01-01 --end-date 2024-01-31
"""
import argparse
import logging
import json
from datetime import datetime
from pathlib import Path

import requests
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_exponential

# Configuration logging JSON
logging.basicConfig(
    level=logging.INFO,
    format='{"time": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s"}'
)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="Ingest data from API")
    parser.add_argument("--start-date", required=True, help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end-date", required=True, help="End date (YYYY-MM-DD)")
    parser.add_argument("--output-dir", default="./data/bronze", help="Output directory")
    parser.add_argument("--page-size", type=int, default=100, help="Items per page")
    return parser.parse_args()
```

**√âtape 2 : Appels API avec retry (25 min)**
```python
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def fetch_page(base_url: str, page: int, page_size: int) -> dict:
    """Fetch une page avec retry exponentiel."""
    url = f"{base_url}?_page={page}&_limit={page_size}"
    logger.info(f"Fetching {url}")
    
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    
    return {
        "data": response.json(),
        "page": page,
        "has_more": len(response.json()) == page_size
    }


def fetch_all_pages(base_url: str, page_size: int = 100) -> list:
    """R√©cup√®re toutes les pages."""
    all_data = []
    page = 1
    
    while True:
        result = fetch_page(base_url, page, page_size)
        all_data.extend(result["data"])
        
        if not result["has_more"]:
            break
        page += 1
    
    logger.info(f"Fetched {len(all_data)} records in {page} pages")
    return all_data
```

**√âtape 3 : √âcriture Parquet partitionn√© (25 min)**
```python
def write_parquet(data: list, output_dir: str, partition_date: str):
    """√âcrit les donn√©es en Parquet partitionn√© par date."""
    if not data:
        logger.warning("No data to write")
        return
    
    df = pd.DataFrame(data)
    
    # Ajouter m√©tadonn√©es
    df["_ingested_at"] = datetime.utcnow().isoformat()
    df["_source"] = "api"
    df["_partition_date"] = partition_date
    
    # Cr√©er r√©pertoire partitionn√©
    output_path = Path(output_dir) / f"partition_date={partition_date}"
    output_path.mkdir(parents=True, exist_ok=True)
    
    # √âcrire avec timestamp pour idempotence
    filename = f"data_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.parquet"
    filepath = output_path / filename
    
    df.to_parquet(filepath, index=False, compression="snappy")
    logger.info(f"Written {len(df)} records to {filepath}")
    
    return str(filepath)
```

**√âtape 4 : Main et orchestration (20 min)**
```python
def main():
    args = parse_args()
    
    logger.info(f"Starting ingestion: {args.start_date} to {args.end_date}")
    
    # API exemple (JSONPlaceholder)
    base_url = "https://jsonplaceholder.typicode.com/posts"
    
    try:
        data = fetch_all_pages(base_url, args.page_size)
        output_file = write_parquet(data, args.output_dir, args.start_date)
        
        # Sortie JSON pour n8n
        result = {
            "status": "success",
            "records_count": len(data),
            "output_file": output_file,
            "ingestion_date": datetime.utcnow().isoformat()
        }
        print(json.dumps(result))
        
    except Exception as e:
        logger.error(f"Ingestion failed: {e}")
        result = {"status": "error", "error": str(e)}
        print(json.dumps(result))
        raise


if __name__ == "__main__":
    main()
```

**√âtape 5 : Int√©gration n8n (30 min)**

Workflow n8n :
```
[Schedule Trigger] ‚Üí [Set Variables] ‚Üí [Execute Command] ‚Üí [Parse JSON] ‚Üí [IF Success] ‚Üí [Log/Notify]
                                                                              ‚Üì Error
                                                                         [Error Notify]
```

Configuration Execute Command :
```bash
python3 /scripts/ingest_api.py \
  --start-date {{ $json.start_date }} \
  --end-date {{ $json.end_date }} \
  --output-dir /data/bronze
```

### 15:30 - 15:45 | ‚òï Pause (15 min)

### 15:45 - 16:45 | TP2.2 : Contr√¥le qualit√© (1h)

#### Script `validate_data.py`

```python
#!/usr/bin/env python3
"""
Validation des donn√©es ing√©r√©es.
Exit code 0 = OK, 1 = Erreurs trouv√©es
"""
import argparse
import json
import sys
from pathlib import Path

import pandas as pd


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input-file", required=True, help="Parquet file to validate")
    parser.add_argument("--rules-file", help="JSON file with validation rules")
    return parser.parse_args()


def validate(df: pd.DataFrame, rules: dict) -> list:
    """Applique les r√®gles de validation."""
    errors = []
    
    # R√®gle 1 : Colonnes requises
    required_cols = rules.get("required_columns", [])
    missing = set(required_cols) - set(df.columns)
    if missing:
        errors.append(f"Missing columns: {missing}")
    
    # R√®gle 2 : Pas de nulls sur certaines colonnes
    not_null_cols = rules.get("not_null_columns", [])
    for col in not_null_cols:
        if col in df.columns:
            null_count = df[col].isna().sum()
            if null_count > 0:
                errors.append(f"Column '{col}' has {null_count} null values")
    
    # R√®gle 3 : Valeurs dans un range
    ranges = rules.get("value_ranges", {})
    for col, (min_val, max_val) in ranges.items():
        if col in df.columns:
            out_of_range = df[(df[col] < min_val) | (df[col] > max_val)]
            if len(out_of_range) > 0:
                errors.append(f"Column '{col}' has {len(out_of_range)} values out of range [{min_val}, {max_val}]")
    
    # R√®gle 4 : Unicit√©
    unique_cols = rules.get("unique_columns", [])
    for col in unique_cols:
        if col in df.columns:
            duplicates = df[col].duplicated().sum()
            if duplicates > 0:
                errors.append(f"Column '{col}' has {duplicates} duplicate values")
    
    # R√®gle 5 : Minimum de lignes
    min_rows = rules.get("min_rows", 0)
    if len(df) < min_rows:
        errors.append(f"Expected at least {min_rows} rows, got {len(df)}")
    
    return errors


def main():
    args = parse_args()
    
    # R√®gles par d√©faut
    default_rules = {
        "required_columns": ["id", "_ingested_at"],
        "not_null_columns": ["id"],
        "unique_columns": ["id"],
        "min_rows": 1
    }
    
    # Charger r√®gles custom si fournies
    if args.rules_file:
        with open(args.rules_file) as f:
            rules = json.load(f)
    else:
        rules = default_rules
    
    # Lire et valider
    df = pd.read_parquet(args.input_file)
    errors = validate(df, rules)
    
    # R√©sultat
    result = {
        "file": args.input_file,
        "rows": len(df),
        "columns": list(df.columns),
        "validation": "passed" if not errors else "failed",
        "errors": errors
    }
    
    print(json.dumps(result, indent=2))
    
    if errors:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()
```

#### Fichier de r√®gles `validation_rules.json`
```json
{
  "required_columns": ["id", "title", "body", "_ingested_at"],
  "not_null_columns": ["id", "title"],
  "unique_columns": ["id"],
  "value_ranges": {
    "userId": [1, 100]
  },
  "min_rows": 10
}
```

#### Int√©gration dans n8n
- Ajouter apr√®s ingestion
- Brancher vers notification si √©chec

### 16:45 - 17:15 | TP2.3 : Backfill (30 min)

#### Ajout mode backfill au workflow n8n

**Concept** :
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Mode Backfill                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Param√®tres:                                                 ‚îÇ
‚îÇ    - start_date: 2024-01-01                                 ‚îÇ
‚îÇ    - end_date: 2024-03-31                                   ‚îÇ
‚îÇ    - mode: backfill                                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Logique:                                                    ‚îÇ
‚îÇ    Pour chaque jour dans [start_date, end_date]:            ‚îÇ
‚îÇ      - V√©rifier si partition existe d√©j√†                    ‚îÇ
‚îÇ      - Si non, ing√©rer                                      ‚îÇ
‚îÇ      - Si oui, skip (idempotence)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Script helper `generate_dates.py`** :
```python
#!/usr/bin/env python3
"""G√©n√®re la liste des dates pour backfill."""
import argparse
import json
from datetime import datetime, timedelta

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--start-date", required=True)
    parser.add_argument("--end-date", required=True)
    args = parser.parse_args()
    
    start = datetime.strptime(args.start_date, "%Y-%m-%d")
    end = datetime.strptime(args.end_date, "%Y-%m-%d")
    
    dates = []
    current = start
    while current <= end:
        dates.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    
    print(json.dumps({"dates": dates}))

if __name__ == "__main__":
    main()
```

**Workflow n8n backfill** :
```
[Manual Trigger] ‚Üí [Set Params] ‚Üí [Execute: generate_dates.py] ‚Üí [Split In Batches] ‚Üí [Execute: ingest_api.py] ‚Üí [Validate]
```

### 17:15 - 17:30 | Wrap-up Jour 2 (15 min)

#### Livrables attendus
- [ ] Script `ingest_api.py` fonctionnel
- [ ] Script `validate_data.py` avec r√®gles
- [ ] Workflow n8n ingestion incr√©mentale
- [ ] Donn√©es bronze en Parquet
- [ ] Documentation dans README

#### R√©capitulatif
```
J2 : Ingestion Incr√©mentale
‚îú‚îÄ‚îÄ Patterns : Full vs Incr√©mental, Watermarks
‚îú‚îÄ‚îÄ Architecture : n8n orchestrateur, Scripts logique
‚îú‚îÄ‚îÄ Formats : JSON ‚Üí Parquet (compression, schema)
‚îú‚îÄ‚îÄ Scripts : ingest_api.py, validate_data.py
‚îú‚îÄ‚îÄ Qualit√© : Validation, DLQ, Alerting
‚îî‚îÄ‚îÄ Ops : Backfill, Idempotence
```

#### Preview Jour 3
> Demain : Airflow ! DAGs, Operators, scheduling production-grade

---

## üìö Ressources

- [Tenacity (retry library)](https://tenacity.readthedocs.io/)
- [Pandas Parquet](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)
- [JSONPlaceholder API](https://jsonplaceholder.typicode.com/)

## ‚ö†Ô∏è Points d'attention formateur

1. **pyarrow** : S'assurer qu'il est install√© (`pip install pyarrow`)
2. **Permissions Docker** : Scripts doivent √™tre ex√©cutables
3. **Partitionnement** : Expliquer le pattern Hive `partition_date=YYYY-MM-DD`
4. **Logs JSON** : Montrer comment les parser (jq, etc.)
