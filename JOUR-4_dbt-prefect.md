# Jour 4 ‚Äî dbt & Prefect : Transformations et Workflows Python

> **Dur√©e totale** : 7h  
> **Objectif** : Ma√Ætriser dbt pour les transformations SQL et d√©couvrir Prefect comme orchestrateur Python-native

---

## üåÖ MATIN (3h30)

### 08:30 - 08:45 | R√©activation Jour 3 (15 min)
- Retour sur les DAGs Airflow
- Questions sur scheduling et backfill
- Introduction : "Le T de ELT"

### 08:45 - 10:15 | Bloc Th√©orique : "ELT moderne et Analytics Engineering" (1h30)

#### Partie 1 : ETL vs ELT (20 min)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ETL (Traditionnel)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  Source ‚îÄ‚îÄ‚ñ∫ [Extract] ‚îÄ‚îÄ‚ñ∫ [Transform] ‚îÄ‚îÄ‚ñ∫ [Load] ‚îÄ‚îÄ‚ñ∫ Warehouse     ‚îÇ
‚îÇ                              ‚ñ≤                                       ‚îÇ
‚îÇ                              ‚îÇ                                       ‚îÇ
‚îÇ                    Transformation AVANT                              ‚îÇ
‚îÇ                    chargement (ETL tools)                           ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  Outils : Informatica, Talend, SSIS                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         ELT (Moderne)                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  Source ‚îÄ‚îÄ‚ñ∫ [Extract] ‚îÄ‚îÄ‚ñ∫ [Load] ‚îÄ‚îÄ‚ñ∫ Warehouse ‚îÄ‚îÄ‚ñ∫ [Transform]     ‚îÇ
‚îÇ                                          ‚ñ≤              ‚îÇ           ‚îÇ
‚îÇ                                          ‚îÇ              ‚îÇ           ‚îÇ
‚îÇ                               Transformation DANS       ‚îÇ           ‚îÇ
‚îÇ                               le warehouse (SQL)        ‚îÇ           ‚îÇ
‚îÇ                                                         ‚ñº           ‚îÇ
‚îÇ                                                   Tables finales    ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  Outils : dbt, Dataform, SQLMesh                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pourquoi ELT aujourd'hui ?**
| Crit√®re | ETL | ELT |
|---------|-----|-----|
| Co√ªt compute | Serveurs ETL d√©di√©s | Warehouse (pay-per-query) |
| Scalabilit√© | Limit√©e | Cloud-native |
| Flexibilit√© | Sch√©ma rigide | Schema-on-read |
| Debugging | Logs ETL tools | SQL transparent |
| Collaboration | Propri√©taire | Git, code review |

#### Partie 2 : Introduction √† dbt (40 min)

**Qu'est-ce que dbt ?**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           dbt (data build tool)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  "dbt permet aux analystes et ing√©nieurs de transformer             ‚îÇ
‚îÇ   les donn√©es dans le warehouse en utilisant des instructions       ‚îÇ
‚îÇ   SELECT simples"                                                   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                       Workflow dbt                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   models/*.sql ‚îÄ‚îÄ‚ñ∫ dbt run ‚îÄ‚îÄ‚ñ∫ Tables/Views dans warehouse   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        ‚îú‚îÄ‚îÄ Jinja templating                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        ‚îú‚îÄ‚îÄ Macros r√©utilisables                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        ‚îú‚îÄ‚îÄ Tests automatiques                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        ‚îî‚îÄ‚îÄ Documentation g√©n√©r√©e                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  Supporte : Snowflake, BigQuery, Redshift, Postgres, DuckDB...     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Concepts cl√©s dbt**

| Concept | Description | Exemple |
|---------|-------------|---------|
| **Source** | Tables brutes (Bronze) | `{{ source('raw', 'events') }}` |
| **Model** | Transformation SQL | `stg_events.sql`, `fact_sales.sql` |
| **Test** | Validation des donn√©es | `unique`, `not_null`, `relationships` |
| **Macro** | Fonction Jinja r√©utilisable | `{{ cents_to_dollars(amount) }}` |
| **Seed** | Donn√©es statiques (CSV) | Mapping codes pays, lookup tables |
| **Snapshot** | Historisation (SCD Type 2) | Suivi des changements |

**Architecture Medallion avec dbt**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Medallion Architecture                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  BRONZE  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  SILVER  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   GOLD   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ   raw_   ‚îÇ     ‚îÇ   stg_   ‚îÇ     ‚îÇ  dim_/   ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ     ‚îÇ          ‚îÇ     ‚îÇ  fact_   ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ       ‚îÇ                ‚îÇ                ‚îÇ                           ‚îÇ
‚îÇ       ‚ñº                ‚ñº                ‚ñº                           ‚îÇ
‚îÇ  ‚Ä¢ Donn√©es brutes  ‚Ä¢ Nettoy√©es      ‚Ä¢ Business-ready              ‚îÇ
‚îÇ  ‚Ä¢ Sch√©ma source   ‚Ä¢ Typ√©es         ‚Ä¢ Agr√©g√©es                    ‚îÇ
‚îÇ  ‚Ä¢ Append-only     ‚Ä¢ D√©dupliqu√©es   ‚Ä¢ Jointes                     ‚îÇ
‚îÇ  ‚Ä¢ Parquet/JSON    ‚Ä¢ Normalis√©es    ‚Ä¢ Document√©es                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Structure projet dbt**
```
dbt_project/
‚îú‚îÄ‚îÄ dbt_project.yml          # Configuration projet
‚îú‚îÄ‚îÄ profiles.yml             # Connexions DB (hors repo)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ staging/             # Silver layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _staging.yml     # Tests & docs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stg_events.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stg_users.sql
‚îÇ   ‚îú‚îÄ‚îÄ marts/               # Gold layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _marts.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_users.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fact_events.sql
‚îÇ   ‚îî‚îÄ‚îÄ sources.yml          # D√©finition des sources
‚îú‚îÄ‚îÄ macros/
‚îÇ   ‚îî‚îÄ‚îÄ custom_macros.sql
‚îú‚îÄ‚îÄ seeds/
‚îÇ   ‚îî‚îÄ‚îÄ country_codes.csv
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ custom_tests.sql
‚îî‚îÄ‚îÄ snapshots/
    ‚îî‚îÄ‚îÄ users_snapshot.sql
```

#### Partie 3 : Introduction √† Prefect (30 min)

**Prefect vs Airflow**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Prefect vs Airflow                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ       Airflow        ‚îÇ              Prefect                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ DAG = fichier s√©par√© ‚îÇ Flow = d√©corateur Python                    ‚îÇ
‚îÇ Operators sp√©cifiques‚îÇ Fonctions Python natives                    ‚îÇ
‚îÇ UI lourde            ‚îÇ UI cloud ou self-hosted                     ‚îÇ
‚îÇ Config YAML/env      ‚îÇ Config Python                               ‚îÇ
‚îÇ Scheduler central    ‚îÇ Agent l√©ger + API                           ‚îÇ
‚îÇ Mature, √©cosyst√®me   ‚îÇ Moderne, DX soign√©e                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Concepts Prefect 2.x**
```python
from prefect import flow, task
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(
    retries=3,
    retry_delay_seconds=60,
    cache_key_fn=task_input_hash,
    cache_expiration=timedelta(hours=1),
)
def extract(source: str) -> dict:
    """Extraction avec retry et cache."""
    # ... logic
    return {"data": [...]}

@task
def transform(data: dict) -> dict:
    """Transformation."""
    return {"processed": [...]}

@task
def load(data: dict, target: str):
    """Chargement."""
    # ... logic

@flow(name="ETL Pipeline", log_prints=True)
def etl_flow(source: str, target: str):
    """Flow principal."""
    raw = extract(source)
    processed = transform(raw)
    load(processed, target)

# Ex√©cution
if __name__ == "__main__":
    etl_flow(source="api", target="warehouse")
```

### 10:15 - 10:30 | ‚òï Pause (15 min)

### 10:30 - 12:00 | D√©monstration : Setup dbt + Prefect (1h30)

#### Setup dbt avec DuckDB (45 min)

**Pourquoi DuckDB pour les TPs ?**
- Z√©ro configuration (fichier local)
- Compatible SQL analytique
- Lit directement Parquet
- Parfait pour le prototypage

**Installation**
```bash
# Installer dbt-core + adapter DuckDB
pip install dbt-core dbt-duckdb

# V√©rifier
dbt --version
```

**Initialiser le projet**
```bash
cd /path/to/formation
dbt init dbt_training

# R√©pondre aux questions :
# - database: duckdb
# - path: data/warehouse.duckdb
```

**Configuration profiles.yml**
```yaml
# ~/.dbt/profiles.yml
dbt_training:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: "{{ env_var('DBT_DATABASE_PATH', 'data/warehouse.duckdb') }}"
      threads: 4
```

**Test connexion**
```bash
cd dbt_training
dbt debug
```

#### Setup Prefect (45 min)

**Installation**
```bash
pip install prefect

# V√©rifier
prefect version
```

**Configuration (optionnel pour le TP)**
```bash
# Mode local (pas besoin de serveur)
prefect config set PREFECT_API_URL=""

# OU avec serveur local
prefect server start  # Terminal 1
prefect config set PREFECT_API_URL="http://localhost:4200/api"
```

**Premier flow**
```python
# flows/hello_prefect.py
from prefect import flow, task

@task
def say_hello(name: str) -> str:
    message = f"Hello, {name}!"
    print(message)
    return message

@flow(name="hello-flow")
def hello_flow(name: str = "World"):
    result = say_hello(name)
    return result

if __name__ == "__main__":
    hello_flow("Prefect")
```

```bash
python flows/hello_prefect.py
```

### 12:00 - 12:15 | Checkpoint matin (15 min)

**Discussion** :
1. Dans quel cas utiliser dbt plut√¥t que Python/Pandas ?
2. Avantages de Prefect sur Airflow pour un data engineer ?
3. Comment dbt s'int√®gre-t-il dans le pipeline global ?

---

## üçΩÔ∏è PAUSE D√âJEUNER (12:15 - 13:30)

---

## üåÜ APR√àS-MIDI (3h30)

### 13:30 - 15:00 | TP4.1 : Projet dbt (1h30)

#### Objectif
> Cr√©er un projet dbt qui transforme les donn√©es Bronze en tables analytiques Silver/Gold

#### √âtape 1 : Configuration sources (15 min)

```yaml
# models/sources.yml
version: 2

sources:
  - name: bronze
    description: "Donn√©es brutes ing√©r√©es depuis l'API"
    schema: main  # Pour DuckDB
    tables:
      - name: raw_events
        description: "√âv√©nements bruts depuis l'API"
        external:
          location: "read_parquet('../data/bronze/**/*.parquet')"
        columns:
          - name: id
            description: "Identifiant unique"
          - name: userId
            description: "ID utilisateur"
          - name: title
            description: "Titre de l'√©v√©nement"
          - name: body
            description: "Corps du message"
          - name: _ingested_at
            description: "Timestamp d'ingestion"
```

#### √âtape 2 : Mod√®le Staging (25 min)

```sql
-- models/staging/stg_events.sql

{{
    config(
        materialized='view',
        tags=['staging', 'daily']
    )
}}

with source as (
    select * from {{ source('bronze', 'raw_events') }}
),

renamed as (
    select
        -- Identifiants
        id as event_id,
        "userId" as user_id,
        
        -- Contenu
        title as event_title,
        body as event_body,
        
        -- M√©tadonn√©es
        _ingested_at as ingested_at,
        
        -- Colonnes calcul√©es
        length(body) as body_length,
        
        -- Audit
        current_timestamp as dbt_updated_at
        
    from source
),

deduplicated as (
    select *,
        row_number() over (
            partition by event_id 
            order by ingested_at desc
        ) as row_num
    from renamed
)

select * from deduplicated
where row_num = 1
```

```yaml
# models/staging/_staging.yml
version: 2

models:
  - name: stg_events
    description: "√âv√©nements nettoy√©s et d√©dupliqu√©s"
    columns:
      - name: event_id
        description: "Identifiant unique de l'√©v√©nement"
        tests:
          - unique
          - not_null
      - name: user_id
        description: "ID de l'utilisateur"
        tests:
          - not_null
      - name: event_title
        description: "Titre nettoy√©"
        tests:
          - not_null
```

#### √âtape 3 : Mod√®les Marts (30 min)

```sql
-- models/marts/dim_users.sql

{{
    config(
        materialized='table',
        tags=['marts', 'daily']
    )
}}

with events as (
    select * from {{ ref('stg_events') }}
),

user_stats as (
    select
        user_id,
        count(*) as total_events,
        min(ingested_at) as first_event_at,
        max(ingested_at) as last_event_at,
        avg(body_length) as avg_body_length
    from events
    group by user_id
)

select
    user_id,
    total_events,
    first_event_at,
    last_event_at,
    avg_body_length,
    
    -- Classification
    case
        when total_events >= 10 then 'power_user'
        when total_events >= 5 then 'active'
        else 'casual'
    end as user_segment,
    
    -- Audit
    current_timestamp as dbt_updated_at

from user_stats
```

```sql
-- models/marts/fact_daily_events.sql

{{
    config(
        materialized='table',
        tags=['marts', 'daily']
    )
}}

with events as (
    select * from {{ ref('stg_events') }}
),

daily_agg as (
    select
        date_trunc('day', ingested_at) as event_date,
        count(*) as event_count,
        count(distinct user_id) as unique_users,
        avg(body_length) as avg_body_length
    from events
    group by 1
)

select
    event_date,
    event_count,
    unique_users,
    avg_body_length,
    
    -- M√©triques d√©riv√©es
    event_count::float / nullif(unique_users, 0) as events_per_user,
    
    -- Audit
    current_timestamp as dbt_updated_at

from daily_agg
order by event_date desc
```

#### √âtape 4 : Ex√©cution et tests (20 min)

```bash
# Compiler sans ex√©cuter (v√©rifier le SQL)
dbt compile

# Ex√©cuter tous les mod√®les
dbt run

# Ex√©cuter uniquement staging
dbt run --select staging.*

# Ex√©cuter un mod√®le et ses d√©pendances
dbt run --select +fact_daily_events

# Lancer les tests
dbt test

# Tests sur un mod√®le sp√©cifique
dbt test --select stg_events
```

### 15:00 - 15:15 | ‚òï Pause (15 min)

### 15:15 - 16:30 | TP4.2 : Orchestration dbt avec Prefect (1h15)

#### Flow Prefect pour dbt

```python
# flows/dbt_flow.py
"""
Orchestration dbt avec Prefect.
Ex√©cute dbt run + dbt test avec gestion des erreurs.
"""
import subprocess
from pathlib import Path
from datetime import datetime

from prefect import flow, task, get_run_logger
from prefect.tasks import task_input_hash

DBT_PROJECT_DIR = Path(__file__).parent.parent / "dbt_training"


@task(retries=2, retry_delay_seconds=30)
def run_dbt_command(command: list[str], project_dir: Path) -> dict:
    """Ex√©cute une commande dbt."""
    logger = get_run_logger()
    
    full_command = ["dbt"] + command + ["--project-dir", str(project_dir)]
    logger.info(f"Running: {' '.join(full_command)}")
    
    result = subprocess.run(
        full_command,
        capture_output=True,
        text=True,
        cwd=project_dir,
    )
    
    # Log output
    if result.stdout:
        logger.info(result.stdout)
    if result.stderr:
        logger.warning(result.stderr)
    
    if result.returncode != 0:
        raise Exception(f"dbt command failed: {result.stderr}")
    
    return {
        "command": command,
        "returncode": result.returncode,
        "stdout": result.stdout,
    }


@task
def dbt_deps(project_dir: Path) -> dict:
    """Installe les d√©pendances dbt."""
    return run_dbt_command(["deps"], project_dir)


@task
def dbt_run(project_dir: Path, select: str = None, full_refresh: bool = False) -> dict:
    """Ex√©cute dbt run."""
    command = ["run"]
    if select:
        command.extend(["--select", select])
    if full_refresh:
        command.append("--full-refresh")
    return run_dbt_command(command, project_dir)


@task
def dbt_test(project_dir: Path, select: str = None) -> dict:
    """Ex√©cute dbt test."""
    command = ["test"]
    if select:
        command.extend(["--select", select])
    return run_dbt_command(command, project_dir)


@task
def dbt_docs_generate(project_dir: Path) -> dict:
    """G√©n√®re la documentation dbt."""
    return run_dbt_command(["docs", "generate"], project_dir)


@flow(name="dbt-pipeline", log_prints=True)
def dbt_pipeline(
    project_dir: Path = DBT_PROJECT_DIR,
    select: str = None,
    run_tests: bool = True,
    generate_docs: bool = False,
    full_refresh: bool = False,
):
    """
    Pipeline dbt complet.
    
    Args:
        project_dir: Chemin vers le projet dbt
        select: S√©lecteur dbt (ex: "staging.*", "+fact_events")
        run_tests: Ex√©cuter les tests apr√®s run
        generate_docs: G√©n√©rer la documentation
        full_refresh: Force full refresh des mod√®les incr√©mentiels
    """
    logger = get_run_logger()
    logger.info(f"Starting dbt pipeline at {datetime.now()}")
    
    # 1. Installer les d√©pendances
    dbt_deps(project_dir)
    
    # 2. Ex√©cuter les mod√®les
    run_result = dbt_run(
        project_dir, 
        select=select, 
        full_refresh=full_refresh
    )
    
    # 3. Tests (optionnel)
    if run_tests:
        test_result = dbt_test(project_dir, select=select)
        logger.info(f"Tests completed: {test_result}")
    
    # 4. Documentation (optionnel)
    if generate_docs:
        dbt_docs_generate(project_dir)
        logger.info("Documentation generated")
    
    logger.info("dbt pipeline completed successfully")
    return {"status": "success", "run_result": run_result}


# Flow de d√©ploiement avec schedule
@flow(name="daily-dbt-pipeline")
def daily_dbt_pipeline():
    """Pipeline quotidien avec configuration fixe."""
    return dbt_pipeline(
        run_tests=True,
        generate_docs=True,
    )


if __name__ == "__main__":
    # Test local
    dbt_pipeline(run_tests=True, generate_docs=True)
```

#### Alternative : Int√©gration Airflow

```python
# dags/dbt_dag.py
"""DAG Airflow pour dbt."""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

DBT_PROJECT_DIR = "/opt/airflow/dbt_training"

default_args = {
    "owner": "data-team",
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="dbt_daily",
    default_args=default_args,
    schedule_interval="0 7 * * *",  # Apr√®s ingestion (6h)
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["dbt", "transformation"],
) as dag:

    dbt_deps = BashOperator(
        task_id="dbt_deps",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt deps",
    )

    dbt_run_staging = BashOperator(
        task_id="dbt_run_staging",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt run --select staging.*",
    )

    dbt_test_staging = BashOperator(
        task_id="dbt_test_staging",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt test --select staging.*",
    )

    dbt_run_marts = BashOperator(
        task_id="dbt_run_marts",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt run --select marts.*",
    )

    dbt_test_marts = BashOperator(
        task_id="dbt_test_marts",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt test --select marts.*",
    )

    dbt_docs = BashOperator(
        task_id="dbt_docs",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt docs generate",
    )

    # Dependencies
    dbt_deps >> dbt_run_staging >> dbt_test_staging >> dbt_run_marts >> dbt_test_marts >> dbt_docs
```

### 16:30 - 17:00 | TP4.3 : Documentation dbt (30 min)

```bash
# G√©n√©rer la documentation
cd dbt_training
dbt docs generate

# Servir localement
dbt docs serve --port 8001

# Ouvrir dans le navigateur
open http://localhost:8001
```

**Explorer la documentation** :
1. **Lineage Graph** : Visualisation des d√©pendances
2. **Model details** : Description, colonnes, tests
3. **Sources** : Connexion aux donn√©es brutes
4. **Exposures** : Utilisation par BI/dashboards

### 17:00 - 17:30 | Wrap-up Jour 4 (30 min)

#### Livrables attendus
- [ ] Projet dbt avec 2-3 mod√®les staging
- [ ] Projet dbt avec 1-2 mod√®les marts
- [ ] Tests dbt configur√©s
- [ ] Flow Prefect ou DAG Airflow pour dbt
- [ ] Documentation dbt g√©n√©r√©e

#### R√©capitulatif
```
J4 : dbt & Prefect
‚îú‚îÄ‚îÄ ELT : Transformation dans le warehouse
‚îú‚îÄ‚îÄ dbt : Sources, Models, Tests, Macros
‚îú‚îÄ‚îÄ Architecture : Bronze ‚Üí Silver ‚Üí Gold
‚îú‚îÄ‚îÄ Prefect : Flows, Tasks, Retry, Cache
‚îú‚îÄ‚îÄ Orchestration : dbt via Prefect/Airflow
‚îî‚îÄ‚îÄ Documentation : Lineage, Column-level
```

#### Preview Jour 5
> Demain : Industrialisation compl√®te - CI/CD, tests, projet fil rouge

---

## üìö Ressources

- [dbt Documentation](https://docs.getdbt.com/)
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)
- [Prefect Documentation](https://docs.prefect.io/)
- [DuckDB + dbt](https://docs.getdbt.com/docs/core/connect-data-platform/duckdb-setup)

## ‚ö†Ô∏è Points d'attention formateur

1. **DuckDB + Parquet** : Bien montrer la lecture directe
2. **profiles.yml** : Ne pas commiter (contient credentials)
3. **Prefect Cloud** : Optionnel, UI locale suffisante pour TP
4. **Tests dbt** : Insister sur leur importance en prod
