# Jour 5 â€” Productionisation, CI/CD & Projet Fil Rouge

> **DurÃ©e totale** : 7h  
> **Objectif** : Industrialiser le pipeline complet et le prÃ©senter lors d'une soutenance

---

## ğŸŒ… MATIN (3h30)

### 08:30 - 08:45 | RÃ©activation Jour 4 (15 min)
- Retour sur dbt et Prefect
- Questions sur l'orchestration
- Introduction : "De la dÃ©mo Ã  la production"

### 08:45 - 10:15 | Bloc ThÃ©orique : "De la dÃ©mo au pipeline exploitable" (1h30)

#### Partie 1 : Qu'est-ce qu'un pipeline "production-ready" ? (20 min)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production Readiness Checklist                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  âœ… VersionnÃ© (Git)           âœ… DocumentÃ©                          â”‚
â”‚  âœ… TestÃ© (unit + intÃ©gration) âœ… Observable (logs, metrics)        â”‚
â”‚  âœ… Reproductible (containers) âœ… SÃ©curisÃ© (secrets, RBAC)          â”‚
â”‚  âœ… Scalable                   âœ… RÃ©silient (retries, alertes)      â”‚
â”‚  âœ… DÃ©ployable (CI/CD)         âœ… Maintenable (code review)         â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  La question clÃ© :                                          â”‚    â”‚
â”‚  â”‚  "Si vous Ãªtes absent, quelqu'un peut-il :                 â”‚    â”‚
â”‚  â”‚   - Comprendre le pipeline ?                               â”‚    â”‚
â”‚  â”‚   - Le debugger ?                                          â”‚    â”‚
â”‚  â”‚   - Le relancer aprÃ¨s un Ã©chec ?"                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Partie 2 : Structure du repository (25 min)

**Arborescence recommandÃ©e**
```
data-pipeline/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml              # Tests + Lint
â”‚       â”œâ”€â”€ cd-staging.yml      # Deploy staging
â”‚       â””â”€â”€ cd-prod.yml         # Deploy production
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ pipeline_bronze.py
â”‚   â”‚   â”œâ”€â”€ pipeline_silver.py
â”‚   â”‚   â””â”€â”€ dbt_dag.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ n8n/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ backfill_trigger.json
â”‚
â”œâ”€â”€ prefect/
â”‚   â””â”€â”€ flows/
â”‚       â””â”€â”€ elt_flow.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_api.py
â”‚   â”œâ”€â”€ validate_data.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logging_config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ test_ingest.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ docker-compose.prod.yml
â”‚   â””â”€â”€ Dockerfile.airflow
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ runbook.md
â”‚   â””â”€â”€ adr/                    # Architecture Decision Records
â”‚       â””â”€â”€ 001-use-airflow.md
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml              # ou requirements.txt
â”œâ”€â”€ Makefile                    # Commandes utiles
â””â”€â”€ README.md
```

**Fichiers de configuration essentiels**

```toml
# pyproject.toml
[project]
name = "data-pipeline"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "apache-airflow>=2.8.0",
    "dbt-core>=1.7.0",
    "dbt-duckdb>=1.7.0",
    "prefect>=2.14.0",
    "pandas>=2.0.0",
    "pyarrow>=14.0.0",
    "requests>=2.31.0",
    "tenacity>=8.2.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
]

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "W"]

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --cov=scripts --cov-report=term-missing"
```

```makefile
# Makefile
.PHONY: install lint test run-airflow run-dbt

install:
	pip install -e ".[dev]"

lint:
	ruff check scripts/ tests/
	ruff format --check scripts/ tests/

format:
	ruff format scripts/ tests/

test:
	pytest tests/unit -v

test-integration:
	pytest tests/integration -v

run-airflow:
	docker-compose -f infra/docker-compose.yml up -d

stop-airflow:
	docker-compose -f infra/docker-compose.yml down

run-dbt:
	cd dbt && dbt run

test-dbt:
	cd dbt && dbt test

docs-dbt:
	cd dbt && dbt docs generate && dbt docs serve
```

#### Partie 3 : Tests et qualitÃ© (25 min)

**Pyramide des tests**
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   /â”‚ E2E     â”‚\
                  / â”‚ Tests   â”‚ \     Moins nombreux
                 /  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \    Mais couvrent
                /                 \   plus de scope
               /   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   \
              /    â”‚Integrationâ”‚    \
             /     â”‚   Tests   â”‚     \
            /      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      \
           /                           \
          /      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \
         /       â”‚    Unit     â”‚         \   Plus nombreux
        /        â”‚   Tests     â”‚          \  Rapides
       /         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           \ CiblÃ©s
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Tests unitaires Python**
```python
# tests/unit/test_ingest.py
import pytest
from scripts.ingest_api import fetch_page, normalize_response

class TestFetchPage:
    def test_successful_fetch(self, mocker):
        """Test une rÃ©cupÃ©ration rÃ©ussie."""
        mock_response = mocker.Mock()
        mock_response.json.return_value = [{"id": 1}]
        mock_response.raise_for_status.return_value = None
        
        mocker.patch("requests.get", return_value=mock_response)
        
        result = fetch_page("http://api.test", page=1, page_size=10)
        
        assert result["data"] == [{"id": 1}]
        assert result["page"] == 1

    def test_retry_on_failure(self, mocker):
        """Test le retry aprÃ¨s Ã©chec."""
        mock_response = mocker.Mock()
        mock_response.raise_for_status.side_effect = [
            Exception("Timeout"),
            Exception("Timeout"),
            None,
        ]
        mock_response.json.return_value = [{"id": 1}]
        
        mocker.patch("requests.get", return_value=mock_response)
        
        result = fetch_page("http://api.test", page=1, page_size=10)
        assert result is not None


class TestNormalizeResponse:
    @pytest.mark.parametrize("input_data,expected", [
        ({"bitcoin_usd": 50000}, {"prices": [{"coin": "bitcoin", "usd": 50000}]}),
        ({}, {"prices": []}),
    ])
    def test_normalization(self, input_data, expected):
        """Test la normalisation avec diffÃ©rentes entrÃ©es."""
        result = normalize_response(input_data)
        assert result["prices"] == expected["prices"]
```

**Tests d'intÃ©gration**
```python
# tests/integration/test_pipeline.py
import subprocess
from pathlib import Path
import pandas as pd
import pytest

DATA_DIR = Path(__file__).parent.parent.parent / "data"

class TestIngestPipeline:
    @pytest.fixture(autouse=True)
    def setup(self, tmp_path):
        """PrÃ©pare un rÃ©pertoire temporaire."""
        self.output_dir = tmp_path / "bronze"
        self.output_dir.mkdir()
        yield
        # Cleanup automatique par pytest

    def test_full_ingestion(self):
        """Test le pipeline d'ingestion complet."""
        result = subprocess.run(
            [
                "python", "scripts/ingest_api.py",
                "--start-date", "2024-01-01",
                "--end-date", "2024-01-01",
                "--output-dir", str(self.output_dir),
            ],
            capture_output=True,
            text=True,
        )
        
        assert result.returncode == 0
        
        # VÃ©rifier le fichier crÃ©Ã©
        parquet_files = list(self.output_dir.rglob("*.parquet"))
        assert len(parquet_files) == 1
        
        # VÃ©rifier le contenu
        df = pd.read_parquet(parquet_files[0])
        assert len(df) > 0
        assert "id" in df.columns
```

**Tests dbt**
```yaml
# dbt/models/staging/_staging.yml
version: 2

models:
  - name: stg_events
    columns:
      - name: event_id
        tests:
          - unique
          - not_null
      - name: user_id
        tests:
          - not_null
          - relationships:
              to: ref('dim_users')
              field: user_id
      - name: event_title
        tests:
          - not_null
          - dbt_expectations.expect_column_value_lengths_to_be_between:
              min_value: 1
              max_value: 500
```

#### Partie 4 : CI/CD avec GitHub Actions (20 min)

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.11"

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install ruff mypy
          pip install -e ".[dev]"
      
      - name: Run Ruff
        run: ruff check scripts/ tests/
      
      - name: Run MyPy
        run: mypy scripts/ --ignore-missing-imports

  test-unit:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: pip install -e ".[dev]"
      
      - name: Run unit tests
        run: pytest tests/unit -v --cov=scripts --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml

  test-dbt:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dbt
        run: pip install dbt-core dbt-duckdb
      
      - name: Run dbt compile
        run: |
          cd dbt
          dbt deps
          dbt compile --profiles-dir .
      
      - name: Run dbt tests
        run: |
          cd dbt
          dbt run --profiles-dir . --target ci
          dbt test --profiles-dir . --target ci

  build:
    runs-on: ubuntu-latest
    needs: [test-unit, test-dbt]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker image
        run: |
          docker build -t data-pipeline:${{ github.sha }} -f infra/Dockerfile.airflow .
      
      - name: Push to registry
        run: |
          # Push vers votre registry (ECR, GCR, etc.)
          echo "Would push to registry here"
```

### 10:15 - 10:30 | â˜• Pause (15 min)

### 10:30 - 12:00 | Bloc pratique : Industrialisation (1h30)

#### Exercice 1 : Logging structurÃ© (20 min)

```python
# scripts/utils/logging_config.py
import logging
import json
from datetime import datetime

class JsonFormatter(logging.Formatter):
    """Formateur JSON pour logs structurÃ©s."""
    
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        
        # Ajouter les extras
        if hasattr(record, "extra"):
            log_entry.update(record.extra)
        
        # Exception si prÃ©sente
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_entry)


def setup_logging(name: str, level: str = "INFO") -> logging.Logger:
    """Configure le logging structurÃ©."""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Handler console
    handler = logging.StreamHandler()
    handler.setFormatter(JsonFormatter())
    logger.addHandler(handler)
    
    return logger


# Utilisation
# logger = setup_logging("ingest_api")
# logger.info("Starting ingestion", extra={"date": "2024-01-01", "source": "api"})
```

#### Exercice 2 : Configuration centralisÃ©e (20 min)

```python
# scripts/config.py
from pathlib import Path
from pydantic_settings import BaseSettings
from pydantic import Field

class Settings(BaseSettings):
    """Configuration centralisÃ©e via variables d'environnement."""
    
    # API
    api_base_url: str = Field(default="https://jsonplaceholder.typicode.com")
    api_timeout: int = Field(default=30)
    api_retries: int = Field(default=3)
    
    # Paths
    data_dir: Path = Field(default=Path("./data"))
    bronze_dir: Path = Field(default=Path("./data/bronze"))
    
    # Database
    db_connection_string: str = Field(default="duckdb:///data/warehouse.duckdb")
    
    # Alerting
    alert_email: str = Field(default="")
    slack_webhook: str = Field(default="")
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"


# Singleton
settings = Settings()
```

```bash
# .env.example
API_BASE_URL=https://jsonplaceholder.typicode.com
API_TIMEOUT=30
DATA_DIR=./data
DB_CONNECTION_STRING=duckdb:///data/warehouse.duckdb
ALERT_EMAIL=team@company.com
SLACK_WEBHOOK=https://hooks.slack.com/...
```

#### Exercice 3 : Runbook (30 min)

```markdown
# docs/runbook.md

# Runbook - Data Pipeline

## Vue d'ensemble

Ce document dÃ©crit les procÃ©dures opÃ©rationnelles pour le pipeline de donnÃ©es.

## Architecture

```
[n8n: Triggers/Backfill] 
        â†“
[Airflow: Orchestration quotidienne]
        â†“
[Scripts Python: Ingestion/Validation]
        â†“
[dbt: Transformations]
        â†“
[DuckDB: Warehouse]
```

## ProcÃ©dures courantes

### Relancer un DAG en Ã©chec

1. AccÃ©der Ã  Airflow UI : http://localhost:8080
2. Naviguer vers le DAG en Ã©chec
3. Identifier la task en Ã©chec (rouge)
4. VÃ©rifier les logs de la task
5. Corriger le problÃ¨me si nÃ©cessaire
6. Click droit sur la task â†’ "Clear" â†’ Confirm

### Backfill historique

```bash
# Via CLI Airflow
airflow dags backfill \
  --start-date 2024-01-01 \
  --end-date 2024-01-31 \
  pipeline_bronze

# Via n8n (webhook)
curl -X POST http://localhost:5678/webhook/backfill \
  -H "Content-Type: application/json" \
  -d '{"start_date": "2024-01-01", "end_date": "2024-01-31"}'
```

### RafraÃ®chir les modÃ¨les dbt

```bash
# Full refresh d'un modÃ¨le
dbt run --select stg_events --full-refresh

# Reconstruire tout
dbt run --full-refresh
dbt test
```

## Alertes et escalade

| Alerte | SÃ©vÃ©ritÃ© | Action |
|--------|----------|--------|
| Ingestion Ã©chouÃ©e | Medium | VÃ©rifier API source, relancer |
| Validation Ã©chouÃ©e | High | Investiguer donnÃ©es, quarantaine |
| dbt tests Ã©chouÃ©s | High | VÃ©rifier qualitÃ© donnÃ©es |
| Pipeline > 2h | Medium | VÃ©rifier performances |

## Contacts

- On-call Data : data-oncall@company.com
- Slack : #data-alerts
```

#### Exercice 4 : Configuration CI (20 min)

- CrÃ©er le fichier `.github/workflows/ci.yml` (voir section prÃ©cÃ©dente)
- Commit et push
- VÃ©rifier l'exÃ©cution sur GitHub

---

## ğŸ½ï¸ PAUSE DÃ‰JEUNER (12:00 - 13:30)

---

## ğŸŒ† APRÃˆS-MIDI : PROJET FIL ROUGE & SOUTENANCES (3h30)

### 13:30 - 15:30 | Consolidation du projet (2h)

#### Checklist de consolidation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHECKLIST PROJET FIL ROUGE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  ğŸ“ Structure repo                                                  â”‚
â”‚  [ ] Arborescence propre                                            â”‚
â”‚  [ ] .gitignore configurÃ©                                           â”‚
â”‚  [ ] .env.example prÃ©sent                                           â”‚
â”‚  [ ] README complet                                                 â”‚
â”‚                                                                      â”‚
â”‚  ğŸ”§ Scripts                                                         â”‚
â”‚  [ ] ingest_api.py fonctionnel                                     â”‚
â”‚  [ ] validate_data.py avec rÃ¨gles                                  â”‚
â”‚  [ ] Logs structurÃ©s                                               â”‚
â”‚  [ ] Configuration externalisÃ©e                                     â”‚
â”‚                                                                      â”‚
â”‚  ğŸ“Š n8n                                                             â”‚
â”‚  [ ] Workflow d'ingestion exportÃ©                                  â”‚
â”‚  [ ] Workflow backfill (optionnel)                                 â”‚
â”‚  [ ] Credentials configurÃ©s                                        â”‚
â”‚                                                                      â”‚
â”‚  ğŸ¯ Airflow                                                         â”‚
â”‚  [ ] DAG pipeline_bronze                                           â”‚
â”‚  [ ] Variables configurÃ©es                                         â”‚
â”‚  [ ] Retry/Error handling                                          â”‚
â”‚                                                                      â”‚
â”‚  ğŸ”„ dbt                                                             â”‚
â”‚  [ ] Au moins 2 modÃ¨les staging                                    â”‚
â”‚  [ ] Au moins 1 modÃ¨le mart                                        â”‚
â”‚  [ ] Tests configurÃ©s                                              â”‚
â”‚  [ ] Documentation gÃ©nÃ©rÃ©e                                         â”‚
â”‚                                                                      â”‚
â”‚  ğŸ³ Infrastructure                                                  â”‚
â”‚  [ ] docker-compose fonctionnel                                    â”‚
â”‚  [ ] Instructions de dÃ©marrage                                     â”‚
â”‚                                                                      â”‚
â”‚  ğŸ“ Documentation                                                   â”‚
â”‚  [ ] README avec setup                                             â”‚
â”‚  [ ] Runbook basique                                               â”‚
â”‚  [ ] Architecture documentÃ©e                                        â”‚
â”‚                                                                      â”‚
â”‚  âœ… Tests                                                           â”‚
â”‚  [ ] Au moins 2 tests unitaires                                    â”‚
â”‚  [ ] Tests dbt passent                                             â”‚
â”‚                                                                      â”‚
â”‚  ğŸš€ CI/CD (bonus)                                                   â”‚
â”‚  [ ] GitHub Actions configurÃ©                                      â”‚
â”‚  [ ] Lint automatique                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Travail en binÃ´me

**RÃ©partition des tÃ¢ches suggÃ©rÃ©e** :

| BinÃ´me membre 1 | BinÃ´me membre 2 |
|-----------------|-----------------|
| Scripts Python + tests | DAGs Airflow |
| Projet dbt | Workflow n8n |
| Documentation | Infrastructure Docker |
| README | Runbook |

### 15:30 - 15:45 | â˜• Pause (15 min)

### 15:45 - 17:15 | Soutenances (1h30)

#### Format soutenance (par groupe)

**DurÃ©e : 15-20 minutes par groupe**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Structure Soutenance                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  ğŸ“Œ Introduction (2 min)                                            â”‚
â”‚     â€¢ PrÃ©sentation du groupe                                        â”‚
â”‚     â€¢ Objectif du pipeline                                          â”‚
â”‚                                                                      â”‚
â”‚  ğŸ–¥ï¸ DÃ©mo live (8-10 min)                                           â”‚
â”‚     â€¢ Workflow n8n : trigger, exÃ©cution, logs                      â”‚
â”‚     â€¢ DAG Airflow : vue graphe, exÃ©cution, XCom                    â”‚
â”‚     â€¢ dbt : run, test, documentation                               â”‚
â”‚     â€¢ DonnÃ©es : montrer Bronze â†’ Silver â†’ Gold                     â”‚
â”‚                                                                      â”‚
â”‚  ğŸ—ï¸ Architecture (3 min)                                           â”‚
â”‚     â€¢ SchÃ©ma global                                                 â”‚
â”‚     â€¢ Choix techniques justifiÃ©s                                    â”‚
â”‚                                                                      â”‚
â”‚  â“ Questions (5 min)                                               â”‚
â”‚     â€¢ Gestion des erreurs ?                                         â”‚
â”‚     â€¢ Comment backfiller ?                                          â”‚
â”‚     â€¢ ScalabilitÃ© ?                                                 â”‚
â”‚     â€¢ AmÃ©liorations futures ?                                       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Grille d'Ã©valuation

| CritÃ¨re | Points | Description |
|---------|--------|-------------|
| **Pipeline fonctionnel** | /30 | Les composants s'exÃ©cutent sans erreur |
| **QualitÃ© du code** | /20 | LisibilitÃ©, structure, bonnes pratiques |
| **Gestion des erreurs** | /15 | Retry, logging, alerting |
| **Documentation** | /15 | README, runbook, dbt docs |
| **PrÃ©sentation** | /10 | ClartÃ©, dÃ©mo fluide |
| **RÃ©ponses aux questions** | /10 | ComprÃ©hension, pertinence |

### 17:15 - 17:30 | ClÃ´ture de la formation (15 min)

#### RÃ©capitulatif des 5 jours

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parcours de la semaine                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  J1: n8n & Fondations                                               â”‚
â”‚      â””â”€â–º Automatisation low-code, triggers, premiers workflows     â”‚
â”‚                                                                      â”‚
â”‚  J2: Ingestion IncrÃ©mentale                                         â”‚
â”‚      â””â”€â–º Patterns, scripts Python, validation, Parquet             â”‚
â”‚                                                                      â”‚
â”‚  J3: Orchestration Airflow                                          â”‚
â”‚      â””â”€â–º DAGs, operators, scheduling, backfill                     â”‚
â”‚                                                                      â”‚
â”‚  J4: dbt & Prefect                                                  â”‚
â”‚      â””â”€â–º Transformations SQL, ELT moderne, orchestration Python    â”‚
â”‚                                                                      â”‚
â”‚  J5: Industrialisation                                              â”‚
â”‚      â””â”€â–º CI/CD, tests, documentation, projet complet               â”‚
â”‚                                                                      â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                      â”‚
â”‚  Vous savez maintenant :                                            â”‚
â”‚  âœ… Construire un pipeline data de bout en bout                    â”‚
â”‚  âœ… Choisir les bons outils selon le contexte                      â”‚
â”‚  âœ… Ã‰crire du code testable et maintenable                         â”‚
â”‚  âœ… Industrialiser avec CI/CD                                      â”‚
â”‚  âœ… Documenter pour la production                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Pour aller plus loin

| Sujet | Ressources |
|-------|------------|
| Airflow avancÃ© | Astronomer Academy, Apache Airflow docs |
| dbt avancÃ© | dbt Learn, Analytics Engineering with dbt |
| Data mesh | Zhamak Dehghani's book |
| DataOps | DataOps Cookbook |
| Streaming | Kafka, Flink, Spark Streaming |
| Orchestration moderne | Dagster, Mage, Kestra |

#### Feedback

- Distribuer formulaire d'Ã©valuation
- Discussion ouverte : points forts, amÃ©liorations
- Ã‰change de contacts LinkedIn/GitHub

---

## ğŸ“š Ressources finales

### Livres recommandÃ©s
- "Fundamentals of Data Engineering" - Joe Reis & Matt Housley
- "The Data Warehouse Toolkit" - Ralph Kimball
- "Data Pipelines Pocket Reference" - James Densmore

### Certifications
- dbt Analytics Engineering Certification
- Astronomer Airflow Certification
- Google Cloud Professional Data Engineer

### CommunautÃ©s
- dbt Slack Community
- Apache Airflow Slack
- Data Engineering subreddit

---

## âš ï¸ Points d'attention formateur

1. **Timing soutenances** : PrÃ©voir du buffer entre les groupes
2. **ProblÃ¨mes techniques** : Avoir un backup dÃ©mo enregistrÃ©
3. **Ã‰valuation** : Rester bienveillant, valoriser les efforts
4. **Feedback** : Collecter Ã  chaud, plus sincÃ¨re
